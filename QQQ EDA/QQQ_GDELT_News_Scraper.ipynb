{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d995bd",
   "metadata": {},
   "source": [
    "# QQQ GDELT News Sentiment Scraper\n",
    "\n",
    "This notebook collects news articles related to QQQ ETF from GDELT (Global Database of Events, Language, and Tone).\n",
    "GDELT provides access to news from around the world and goes back much further than Reddit data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f66311",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb0b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdeltdoc\n",
      "  Downloading gdeltdoc-1.12.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.13.3)\n",
      "Collecting typing-extensions>=4.13.0 (from gdeltdoc)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading gdeltdoc-1.12.0-py3-none-any.whl (17 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: typing-extensions, gdeltdoc\n",
      "\n",
      "  Attempting uninstall: typing-extensions\n",
      "\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 0/2 [typing-extensions]\n",
      "   ---------------------------------------- 2/2 [gdeltdoc]\n",
      "\n",
      "Successfully installed gdeltdoc-1.12.0 typing-extensions-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\aarit\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install gdeltdoc pandas vaderSentiment requests beautifulsoup4\n",
    "\n",
    "# Import required libraries\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9591a1",
   "metadata": {},
   "source": [
    "## 2. Define QQQ News Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4409dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_qqq_news(start_date, end_date, max_records=1000):\n",
    "    \"\"\"\n",
    "    Scrape QQQ-related news from GDELT\n",
    "    Args:\n",
    "        start_date (str): Start date in format 'YYYY-MM-DD'\n",
    "        end_date (str): End date in format 'YYYY-MM-DD'\n",
    "        max_records (int): Maximum number of records to retrieve\n",
    "    Returns:\n",
    "        DataFrame: Processed news data with sentiment scores\n",
    "    \"\"\"\n",
    "    # Initialize GDELT\n",
    "    gd = GdeltDoc()\n",
    "    \n",
    "    # Define search terms for QQQ\n",
    "    search_terms = ['QQQ ETF', 'Invesco QQQ', 'NASDAQ-100 ETF', 'QQQ Trust']\n",
    "    \n",
    "    all_articles = []\n",
    "    \n",
    "    print(f\"Collecting QQQ news from {start_date} to {end_date}\")\n",
    "    \n",
    "    for term in search_terms:\n",
    "        try:\n",
    "            print(f\"\\nSearching for: {term}...\")\n",
    "            \n",
    "            # Create filters\n",
    "            f = Filters(\n",
    "                keyword=term,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date\n",
    "            )\n",
    "            \n",
    "            # Get articles (removed max_recursion_depth parameter)\n",
    "            articles = gd.article_search(f)\n",
    "            \n",
    "            if articles is not None and len(articles) > 0:\n",
    "                articles['search_term'] = term\n",
    "                all_articles.append(articles)\n",
    "                print(f\"Found {len(articles)} articles for '{term}'\")\n",
    "            else:\n",
    "                print(f\"No articles found for '{term}'\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{term}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_articles:\n",
    "        print(\"No articles found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all articles\n",
    "    news_df = pd.concat(all_articles, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates based on URL\n",
    "    news_df = news_df.drop_duplicates(subset=['url'], keep='first')\n",
    "    \n",
    "    # Limit to max_records\n",
    "    if len(news_df) > max_records:\n",
    "        news_df = news_df.head(max_records)\n",
    "    \n",
    "    print(f\"\\nTotal unique articles: {len(news_df)}\")\n",
    "    \n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9592c1f",
   "metadata": {},
   "source": [
    "## 3. Add Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4603c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_scores(df):\n",
    "    \"\"\"\n",
    "    Add VADER sentiment scores to news articles\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_sentiment(text):\n",
    "        if pd.isna(text) or text == '':\n",
    "            return pd.Series({'compound': 0, 'pos': 0, 'neu': 0, 'neg': 0})\n",
    "        scores = analyzer.polarity_scores(text)\n",
    "        return pd.Series(scores)\n",
    "    \n",
    "    print(\"Calculating sentiment scores...\")\n",
    "    \n",
    "    # Calculate sentiment on title\n",
    "    sentiment = df['title'].apply(get_sentiment)\n",
    "    df = pd.concat([df, sentiment], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a58019",
   "metadata": {},
   "source": [
    "## 4. Execute Data Collection\n",
    "\n",
    "Set your desired date range below. GDELT data goes back several years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec72d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from 2025-05-14 to 2025-11-10\n",
      "Collecting QQQ news from 2025-05-14 to 2025-11-10\n",
      "\n",
      "Searching for: QQQ ETF...\n",
      "Found 128 articles for 'QQQ ETF'\n",
      "Found 128 articles for 'QQQ ETF'\n",
      "\n",
      "Searching for: Invesco QQQ...\n",
      "\n",
      "Searching for: Invesco QQQ...\n",
      "Found 250 articles for 'Invesco QQQ'\n",
      "Found 250 articles for 'Invesco QQQ'\n",
      "\n",
      "Searching for: NASDAQ-100 ETF...\n",
      "\n",
      "Searching for: NASDAQ-100 ETF...\n",
      "Found 76 articles for 'NASDAQ-100 ETF'\n",
      "Found 76 articles for 'NASDAQ-100 ETF'\n",
      "\n",
      "Searching for: QQQ Trust...\n",
      "\n",
      "Searching for: QQQ Trust...\n",
      "Found 250 articles for 'QQQ Trust'\n",
      "Found 250 articles for 'QQQ Trust'\n",
      "\n",
      "Total unique articles: 535\n",
      "Calculating sentiment scores...\n",
      "\n",
      "Collection complete! Found 535 articles\n",
      "\n",
      "Sample of collected data:\n",
      "                       date  \\\n",
      "0 2025-10-30 08:45:00+00:00   \n",
      "1 2025-10-19 10:45:00+00:00   \n",
      "2 2025-09-15 14:15:00+00:00   \n",
      "3 2025-09-05 14:00:00+00:00   \n",
      "4 2025-09-10 20:30:00+00:00   \n",
      "\n",
      "                                               title             domain  \\\n",
      "0  The Stock Market Is Doing Something for the 7t...           fool.com   \n",
      "1  The Nasdaq Is Doing Something Seen 7 Times Sin...  finance.yahoo.com   \n",
      "2  Can the QQQ ETF Protect Your Income in a Volat...           fool.com   \n",
      "3  If Youd Invested $1 , 000 in the Invesco QQQ T...           fool.com   \n",
      "4  QQQ ETF stock : Is the Nasdaq 100 Index at ris...         invezz.com   \n",
      "\n",
      "   compound  \n",
      "0    0.0000  \n",
      "1    0.0000  \n",
      "2    0.3818  \n",
      "3    0.5106  \n",
      "4   -0.2732  \n",
      "\n",
      "Total unique articles: 535\n",
      "Calculating sentiment scores...\n",
      "\n",
      "Collection complete! Found 535 articles\n",
      "\n",
      "Sample of collected data:\n",
      "                       date  \\\n",
      "0 2025-10-30 08:45:00+00:00   \n",
      "1 2025-10-19 10:45:00+00:00   \n",
      "2 2025-09-15 14:15:00+00:00   \n",
      "3 2025-09-05 14:00:00+00:00   \n",
      "4 2025-09-10 20:30:00+00:00   \n",
      "\n",
      "                                               title             domain  \\\n",
      "0  The Stock Market Is Doing Something for the 7t...           fool.com   \n",
      "1  The Nasdaq Is Doing Something Seen 7 Times Sin...  finance.yahoo.com   \n",
      "2  Can the QQQ ETF Protect Your Income in a Volat...           fool.com   \n",
      "3  If Youd Invested $1 , 000 in the Invesco QQQ T...           fool.com   \n",
      "4  QQQ ETF stock : Is the Nasdaq 100 Index at ris...         invezz.com   \n",
      "\n",
      "   compound  \n",
      "0    0.0000  \n",
      "1    0.0000  \n",
      "2    0.3818  \n",
      "3    0.5106  \n",
      "4   -0.2732  \n"
     ]
    }
   ],
   "source": [
    "# Set date range (you can go back several years with GDELT)\n",
    "# Example: Last 6 months\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=180)  # 6 months\n",
    "\n",
    "# Format dates for GDELT\n",
    "start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Fetching news from {start_date_str} to {end_date_str}\")\n",
    "\n",
    "# Collect news data\n",
    "news_df = scrape_qqq_news(start_date_str, end_date_str, max_records=2000)\n",
    "\n",
    "# Add sentiment scores\n",
    "if len(news_df) > 0:\n",
    "    news_df = add_sentiment_scores(news_df)\n",
    "    \n",
    "    # Convert seendate to datetime\n",
    "    news_df['date'] = pd.to_datetime(news_df['seendate'])\n",
    "    \n",
    "    print(f\"\\nCollection complete! Found {len(news_df)} articles\")\n",
    "    print(f\"\\nSample of collected data:\")\n",
    "    print(news_df[['date', 'title', 'domain', 'compound']].head())\n",
    "else:\n",
    "    print(\"No data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b837aa9",
   "metadata": {},
   "source": [
    "## 5. Aggregate and Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbbe940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed data saved to QQQ_gdelt_news_20251110_194823.csv\n",
      "Daily summary saved to QQQ_gdelt_daily_sentiment_20251110_194823.csv\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total articles: 535\n",
      "Date range: 2025-08-12 to 2025-11-11\n",
      "Average daily sentiment: 0.1889\n",
      "Average articles per day: 5.9\n",
      "\n",
      "Daily sentiment summary:\n",
      "        compound        pos        neg        neu  article_count\n",
      "count  91.000000  91.000000  91.000000  91.000000      91.000000\n",
      "mean    0.188853   0.118765   0.033807   0.847418       5.879121\n",
      "std     0.179530   0.071819   0.038412   0.074319       3.161698\n",
      "min    -0.273200   0.000000   0.000000   0.665750       1.000000\n",
      "25%     0.073233   0.075833   0.000000   0.805167       3.000000\n",
      "50%     0.196492   0.121615   0.025167   0.847889       6.000000\n",
      "75%     0.312396   0.152950   0.050556   0.886562       7.000000\n",
      "max     0.571000   0.334250   0.231000   1.000000      14.000000\n"
     ]
    }
   ],
   "source": [
    "if len(news_df) > 0:\n",
    "    # Save detailed data\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    detailed_filename = f\"QQQ_gdelt_news_{timestamp}.csv\"\n",
    "    news_df.to_csv(detailed_filename, index=False)\n",
    "    print(f\"Detailed data saved to {detailed_filename}\")\n",
    "    \n",
    "    # Aggregate by date\n",
    "    daily_sentiment = news_df.copy()\n",
    "    daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date']).map(lambda x: x.date())\n",
    "    \n",
    "    daily_summary = daily_sentiment.groupby('date').agg({\n",
    "        'compound': 'mean',\n",
    "        'pos': 'mean',\n",
    "        'neg': 'mean',\n",
    "        'neu': 'mean',\n",
    "        'title': 'count'  # Number of articles per day\n",
    "    }).reset_index()\n",
    "    \n",
    "    daily_summary.columns = ['date', 'compound', 'pos', 'neg', 'neu', 'article_count']\n",
    "    \n",
    "    # Save aggregated data\n",
    "    summary_filename = f\"QQQ_gdelt_daily_sentiment_{timestamp}.csv\"\n",
    "    daily_summary.to_csv(summary_filename, index=False)\n",
    "    print(f\"Daily summary saved to {summary_filename}\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\n=== Summary Statistics ===\")\n",
    "    print(f\"Total articles: {len(news_df)}\")\n",
    "    print(f\"Date range: {daily_summary['date'].min()} to {daily_summary['date'].max()}\")\n",
    "    print(f\"Average daily sentiment: {daily_summary['compound'].mean():.4f}\")\n",
    "    print(f\"Average articles per day: {daily_summary['article_count'].mean():.1f}\")\n",
    "    print(\"\\nDaily sentiment summary:\")\n",
    "    print(daily_summary.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33db0c",
   "metadata": {},
   "source": [
    "## 6. Merge with Technical Data\n",
    "\n",
    "Combine GDELT sentiment data with QQQ technical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a332f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged data saved to QQQ_technical_gdelt_sentiment_20251110_194823.csv\n",
      "\n",
      "Sample of merged data:\n",
      "         Date        Open        High         Low       Close    Volume  \\\n",
      "0  2023-11-03  360.320764  364.538902  360.093546  363.244812  53280500   \n",
      "1  2023-11-06  364.015365  365.289709  362.454565  364.726624  38848700   \n",
      "2  2023-11-07  365.773740  369.043544  364.568554  368.174255  50777400   \n",
      "3  2023-11-08  368.549600  369.251000  366.119481  368.411316  35663400   \n",
      "4  2023-11-09  369.102828  370.248715  365.082243  365.576172  53859400   \n",
      "\n",
      "   Dividends  Stock Splits  Capital Gains  Daily_Return  ...  Volume_MA_10  \\\n",
      "0        0.0           0.0            0.0           NaN  ...           NaN   \n",
      "1        0.0           0.0            0.0      0.004079  ...           NaN   \n",
      "2        0.0           0.0            0.0      0.009453  ...           NaN   \n",
      "3        0.0           0.0            0.0      0.000644  ...           NaN   \n",
      "4        0.0           0.0            0.0     -0.007696  ...           NaN   \n",
      "\n",
      "   Volume_Ratio  High_20d  Low_20d  Price_Position  compound  pos  neg  neu  \\\n",
      "0           NaN       NaN      NaN             NaN       0.0  0.0  0.0  0.0   \n",
      "1           NaN       NaN      NaN             NaN       0.0  0.0  0.0  0.0   \n",
      "2           NaN       NaN      NaN             NaN       0.0  0.0  0.0  0.0   \n",
      "3           NaN       NaN      NaN             NaN       0.0  0.0  0.0  0.0   \n",
      "4           NaN       NaN      NaN             NaN       0.0  0.0  0.0  0.0   \n",
      "\n",
      "   article_count  \n",
      "0            0.0  \n",
      "1            0.0  \n",
      "2            0.0  \n",
      "3            0.0  \n",
      "4            0.0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarit\\AppData\\Local\\Temp\\ipykernel_43800\\3344644005.py:4: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  technical_data['Date'] = pd.to_datetime(technical_data['Date']).map(lambda x: x.date())\n"
     ]
    }
   ],
   "source": [
    "if len(news_df) > 0:\n",
    "    # Load technical data\n",
    "    technical_data = pd.read_csv(\"QQQ_Historical_DayByDay.csv\")\n",
    "    technical_data['Date'] = pd.to_datetime(technical_data['Date']).map(lambda x: x.date())\n",
    "    \n",
    "    # Merge with sentiment data\n",
    "    merged_data = technical_data.merge(\n",
    "        daily_summary,\n",
    "        left_on='Date',\n",
    "        right_on='date',\n",
    "        how='left'\n",
    "    ).drop('date', axis=1)\n",
    "    \n",
    "    # Fill missing sentiment values with 0\n",
    "    sentiment_columns = ['compound', 'pos', 'neg', 'neu', 'article_count']\n",
    "    merged_data[sentiment_columns] = merged_data[sentiment_columns].fillna(0)\n",
    "    \n",
    "    # Save merged data\n",
    "    merged_filename = f\"QQQ_technical_gdelt_sentiment_{timestamp}.csv\"\n",
    "    merged_data.to_csv(merged_filename, index=False)\n",
    "    print(f\"\\nMerged data saved to {merged_filename}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample of merged data:\")\n",
    "    print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73885dd",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### GDELT Advantages:\n",
    "- Historical data going back several years\n",
    "- Global news coverage\n",
    "- Multiple languages\n",
    "- High-quality financial news sources\n",
    "\n",
    "### Tips:\n",
    "- Start with smaller date ranges to test\n",
    "- GDELT has rate limits, so add delays between requests\n",
    "- You can adjust the date range to get more historical data\n",
    "- Consider running multiple date ranges if you need extensive historical data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
