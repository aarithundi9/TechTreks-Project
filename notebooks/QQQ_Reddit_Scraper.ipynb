{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2c4859",
   "metadata": {},
   "source": [
    "# QQQ Reddit Sentiment Scraper\n",
    "\n",
    "This notebook collects Reddit posts and comments related to QQQ ETF from the past month. It uses the PRAW library to interact with Reddit's API and pandas for data organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4de7b1",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f51387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (7.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.3.2)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aarit\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\aarit\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install praw pandas vaderSentiment\n",
    "\n",
    "# Import required libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a759fb",
   "metadata": {},
   "source": [
    "## 2. Configure Reddit API\n",
    "\n",
    "Replace the placeholders below with your Reddit API credentials. You can get these by:\n",
    "1. Going to https://www.reddit.com/prefs/apps\n",
    "2. Creating a new app (choose \"script\" type)\n",
    "3. Copy the client_id and client_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182b5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API Configuration\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"x6_wNje4h80DiBvs-Ly6Kw\",\n",
    "    client_secret=\"PTk8OvQaUcyyq9n0QUhWD9PnpqrgKA\",\n",
    "    user_agent=\"QQQ_Sentiment_Analysis_Bot/1.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c0fed",
   "metadata": {},
   "source": [
    "## 3. Define QQQ Data Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c858b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_qqq_data(days=30):\n",
    "    \"\"\"\n",
    "    Scrape QQQ-related posts from Reddit\n",
    "    Args:\n",
    "        days (int): Number of days of historical data to collect\n",
    "    Returns:\n",
    "        DataFrame: Processed Reddit posts data\n",
    "    \"\"\"\n",
    "    # Define relevant subreddits and search terms\n",
    "    subreddits = ['investing', 'stocks', 'wallstreetbets', 'StockMarket', 'ETFs']\n",
    "    search_terms = ['QQQ', 'NASDAQ-100', 'Invesco QQQ Trust']\n",
    "    \n",
    "    # Calculate date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    # Initialize data collection\n",
    "    posts_data = []\n",
    "    \n",
    "    print(f\"Collecting QQQ posts from {start_date.date()} to {end_date.date()}\")\n",
    "    \n",
    "    # Scrape data from each subreddit\n",
    "    for sub_name in subreddits:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(sub_name)\n",
    "            print(f\"\\nScraping r/{sub_name}...\")\n",
    "            \n",
    "            for term in search_terms:\n",
    "                for post in subreddit.search(term, limit=100, sort='new'):\n",
    "                    post_date = datetime.fromtimestamp(post.created_utc)\n",
    "                    \n",
    "                    if start_date <= post_date <= end_date:\n",
    "                        # Extract post data\n",
    "                        post_data = {\n",
    "                            'date': post_date,\n",
    "                            'subreddit': sub_name,\n",
    "                            'title': post.title,\n",
    "                            'text': post.selftext,\n",
    "                            'score': post.score,\n",
    "                            'num_comments': post.num_comments,\n",
    "                            'upvote_ratio': post.upvote_ratio,\n",
    "                            'url': f\"https://reddit.com{post.permalink}\"\n",
    "                        }\n",
    "                        \n",
    "                        # Add sentiment analysis\n",
    "                        analyzer = SentimentIntensityAnalyzer()\n",
    "                        sentiment = analyzer.polarity_scores(post.title + ' ' + post.selftext)\n",
    "                        post_data.update(sentiment)\n",
    "                        \n",
    "                        posts_data.append(post_data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping r/{sub_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(posts_data)\n",
    "    \n",
    "    print(f\"\\nCollection complete! Found {len(df)} posts\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d2273",
   "metadata": {},
   "source": [
    "## 4. Execute Data Collection and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bafcfc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting QQQ posts from 2025-10-04 to 2025-11-03\n",
      "\n",
      "Scraping r/investing...\n",
      "\n",
      "Scraping r/stocks...\n",
      "\n",
      "Scraping r/stocks...\n",
      "\n",
      "Scraping r/wallstreetbets...\n",
      "\n",
      "Scraping r/wallstreetbets...\n",
      "\n",
      "Scraping r/StockMarket...\n",
      "\n",
      "Scraping r/StockMarket...\n",
      "\n",
      "Scraping r/ETFs...\n",
      "\n",
      "Scraping r/ETFs...\n",
      "\n",
      "Collection complete! Found 199 posts\n",
      "\n",
      "Data saved to QQQ_reddit_data_20251103_170415.csv\n",
      "\n",
      "Sample of collected data:\n",
      "                 date  subreddit  \\\n",
      "0 2025-11-03 02:41:34  investing   \n",
      "1 2025-11-01 18:24:49  investing   \n",
      "2 2025-11-01 13:36:05  investing   \n",
      "3 2025-10-31 19:05:30  investing   \n",
      "4 2025-10-31 18:37:50  investing   \n",
      "\n",
      "                                               title  score  compound  \n",
      "0  Portfolio Planning Advice - Roth IRA vs Brokerage      1    0.9724  \n",
      "1  For those who invest all savings after emergen...     25    0.9320  \n",
      "2     Best instrument to express a bearish TSLA view      0    0.2886  \n",
      "3       It's never a bad idea to take profits right?    172    0.7037  \n",
      "4                        Diversifying outside of SPY      0    0.8455  \n",
      "\n",
      "Collection complete! Found 199 posts\n",
      "\n",
      "Data saved to QQQ_reddit_data_20251103_170415.csv\n",
      "\n",
      "Sample of collected data:\n",
      "                 date  subreddit  \\\n",
      "0 2025-11-03 02:41:34  investing   \n",
      "1 2025-11-01 18:24:49  investing   \n",
      "2 2025-11-01 13:36:05  investing   \n",
      "3 2025-10-31 19:05:30  investing   \n",
      "4 2025-10-31 18:37:50  investing   \n",
      "\n",
      "                                               title  score  compound  \n",
      "0  Portfolio Planning Advice - Roth IRA vs Brokerage      1    0.9724  \n",
      "1  For those who invest all savings after emergen...     25    0.9320  \n",
      "2     Best instrument to express a bearish TSLA view      0    0.2886  \n",
      "3       It's never a bad idea to take profits right?    172    0.7037  \n",
      "4                        Diversifying outside of SPY      0    0.8455  \n"
     ]
    }
   ],
   "source": [
    "# Collect QQQ Reddit data\n",
    "qqq_posts = scrape_qqq_data(days=30)\n",
    "\n",
    "# Save data to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"QQQ_reddit_data_{timestamp}.csv\"\n",
    "qqq_posts.to_csv(filename, index=False)\n",
    "print(f\"\\nData saved to {filename}\")\n",
    "\n",
    "# Display sample of collected data\n",
    "print(\"\\nSample of collected data:\")\n",
    "print(qqq_posts[['date', 'subreddit', 'title', 'score', 'compound']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460cfb1",
   "metadata": {},
   "source": [
    "## 5. Merge Sentiment Data with Technical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75afe662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged data saved to QQQ_technical_sentiment_20251103_170415.csv\n",
      "\n",
      "Sample of merged data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarit\\AppData\\Local\\Temp\\ipykernel_21532\\1954935240.py:4: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  technical_data['Date'] = pd.to_datetime(technical_data['Date']).map(lambda x: x.date())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Capital Gains</th>\n",
       "      <th>Daily_Return</th>\n",
       "      <th>...</th>\n",
       "      <th>Volume_Ratio</th>\n",
       "      <th>High_20d</th>\n",
       "      <th>Low_20d</th>\n",
       "      <th>Price_Position</th>\n",
       "      <th>compound</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>608.450012</td>\n",
       "      <td>609.359985</td>\n",
       "      <td>605.969971</td>\n",
       "      <td>607.710022</td>\n",
       "      <td>41962100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816829</td>\n",
       "      <td>609.359985</td>\n",
       "      <td>576.371816</td>\n",
       "      <td>0.949983</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.816800</td>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-07</td>\n",
       "      <td>609.020020</td>\n",
       "      <td>609.710022</td>\n",
       "      <td>603.030029</td>\n",
       "      <td>604.510010</td>\n",
       "      <td>58209500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>...</td>\n",
       "      <td>1.147453</td>\n",
       "      <td>609.710022</td>\n",
       "      <td>577.880053</td>\n",
       "      <td>0.836632</td>\n",
       "      <td>0.222786</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.053286</td>\n",
       "      <td>0.878143</td>\n",
       "      <td>18</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-08</td>\n",
       "      <td>605.409973</td>\n",
       "      <td>611.750000</td>\n",
       "      <td>605.260010</td>\n",
       "      <td>611.440002</td>\n",
       "      <td>50629800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996507</td>\n",
       "      <td>611.750000</td>\n",
       "      <td>580.946513</td>\n",
       "      <td>0.989936</td>\n",
       "      <td>0.383325</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.892750</td>\n",
       "      <td>54</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-09</td>\n",
       "      <td>611.479980</td>\n",
       "      <td>611.609985</td>\n",
       "      <td>607.479980</td>\n",
       "      <td>610.700012</td>\n",
       "      <td>45551000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943664</td>\n",
       "      <td>611.750000</td>\n",
       "      <td>583.423619</td>\n",
       "      <td>0.962933</td>\n",
       "      <td>0.134686</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.056143</td>\n",
       "      <td>0.906143</td>\n",
       "      <td>591</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-10</td>\n",
       "      <td>611.400024</td>\n",
       "      <td>613.179993</td>\n",
       "      <td>589.049988</td>\n",
       "      <td>589.500000</td>\n",
       "      <td>97614800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.034714</td>\n",
       "      <td>...</td>\n",
       "      <td>1.855862</td>\n",
       "      <td>613.179993</td>\n",
       "      <td>583.693348</td>\n",
       "      <td>0.196925</td>\n",
       "      <td>0.223386</td>\n",
       "      <td>0.092571</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.855357</td>\n",
       "      <td>5050</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close    Volume  \\\n",
       "0  2025-10-06  608.450012  609.359985  605.969971  607.710022  41962100   \n",
       "1  2025-10-07  609.020020  609.710022  603.030029  604.510010  58209500   \n",
       "2  2025-10-08  605.409973  611.750000  605.260010  611.440002  50629800   \n",
       "3  2025-10-09  611.479980  611.609985  607.479980  610.700012  45551000   \n",
       "4  2025-10-10  611.400024  613.179993  589.049988  589.500000  97614800   \n",
       "\n",
       "   Dividends  Stock Splits  Capital Gains  Daily_Return  ...  Volume_Ratio  \\\n",
       "0        0.0           0.0            0.0      0.007510  ...      0.816829   \n",
       "1        0.0           0.0            0.0     -0.005266  ...      1.147453   \n",
       "2        0.0           0.0            0.0      0.011464  ...      0.996507   \n",
       "3        0.0           0.0            0.0     -0.001210  ...      0.943664   \n",
       "4        0.0           0.0            0.0     -0.034714  ...      1.855862   \n",
       "\n",
       "     High_20d     Low_20d  Price_Position  compound       pos       neg  \\\n",
       "0  609.359985  576.371816        0.949983  0.603900  0.129200  0.054200   \n",
       "1  609.710022  577.880053        0.836632  0.222786  0.068571  0.053286   \n",
       "2  611.750000  580.946513        0.989936  0.383325  0.082000  0.025500   \n",
       "3  611.750000  583.423619        0.962933  0.134686  0.038000  0.056143   \n",
       "4  613.179993  583.693348        0.196925  0.223386  0.092571  0.052000   \n",
       "\n",
       "        neu  score  num_comments  \n",
       "0  0.816800     99            96  \n",
       "1  0.878143     18           122  \n",
       "2  0.892750     54            79  \n",
       "3  0.906143    591           865  \n",
       "4  0.855357   5050           797  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load technical data\n",
    "technical_data = pd.read_csv(\"../data/QQQ_Historical_DayByDay.csv\")\n",
    "# Convert to just the date using map\n",
    "technical_data['Date'] = pd.to_datetime(technical_data['Date']).map(lambda x: x.date())\n",
    "\n",
    "# Aggregate sentiment data by date\n",
    "daily_sentiment = qqq_posts.copy()\n",
    "# Convert to just the date using map\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date']).map(lambda x: x.date())\n",
    "daily_sentiment = daily_sentiment.groupby('date').agg({\n",
    "    'compound': 'mean',\n",
    "    'pos': 'mean',\n",
    "    'neg': 'mean',\n",
    "    'neu': 'mean',\n",
    "    'score': 'sum',\n",
    "    'num_comments': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Get the last month cutoff date\n",
    "last_month = datetime.now().date()\n",
    "last_month = last_month - timedelta(days=30)\n",
    "\n",
    "# Filter technical data for last month\n",
    "recent_technical = technical_data[technical_data['Date'] >= last_month].copy()\n",
    "\n",
    "# Merge technical and sentiment data\n",
    "merged_data = recent_technical.merge(\n",
    "    daily_sentiment,\n",
    "    left_on='Date',\n",
    "    right_on='date',\n",
    "    how='left'\n",
    ").drop('date', axis=1)\n",
    "\n",
    "# Fill missing sentiment values with 0 (days with no Reddit posts)\n",
    "sentiment_columns = ['compound', 'pos', 'neg', 'neu', 'score', 'num_comments']\n",
    "merged_data[sentiment_columns] = merged_data[sentiment_columns].fillna(0)\n",
    "\n",
    "# Save merged data\n",
    "merged_filename = f\"QQQ_technical_sentiment_{timestamp}.csv\"\n",
    "merged_data.to_csv(merged_filename, index=False)\n",
    "print(f\"\\nMerged data saved to {merged_filename}\")\n",
    "\n",
    "# Display sample of merged data\n",
    "print(\"\\nSample of merged data:\")\n",
    "display(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69557b",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The merged dataset now contains:\n",
    "\n",
    "Technical Data:\n",
    "- Date: Trading date\n",
    "- Open, High, Low, Close: Daily price data\n",
    "- Volume: Trading volume\n",
    "- Adj Close: Adjusted closing price\n",
    "\n",
    "Sentiment Data:\n",
    "- compound: Overall sentiment score (-1 to 1)\n",
    "- pos: Positive sentiment score (0 to 1)\n",
    "- neg: Negative sentiment score (0 to 1)\n",
    "- neu: Neutral sentiment score (0 to 1)\n",
    "- score: Sum of Reddit post scores for the day\n",
    "- num_comments: Total number of comments on QQQ posts for the day\n",
    "\n",
    "Note: Days without any Reddit posts will have sentiment scores of 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
